"""
ğŸ¤– AI OMR - Scalable Version for Large Classes (500-700 students)
Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹ Ù„Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
"""
import io, base64, time, gc
from dataclasses import dataclass
from typing import Dict, List, Optional
import cv2, numpy as np, pandas as pd
import streamlit as st
from pdf2image import convert_from_bytes
from PIL import Image
from datetime import datetime

# Same helper functions...
def read_bytes(f):
    if not f: return b""
    try: return f.getbuffer().tobytes()
    except: 
        try: return f.read()
        except: return b""

def load_pages(file_bytes, filename, dpi=200):  # Lower DPI for speed
    """Load pages with memory management"""
    if filename.lower().endswith(".pdf"):
        pages = convert_from_bytes(file_bytes, dpi=dpi)
        return [p.convert("RGB") for p in pages]
    return [Image.open(io.BytesIO(file_bytes)).convert("RGB")]

def pil_to_bgr(pil_img):
    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)

def bgr_to_bytes(bgr):
    _, buffer = cv2.imencode('.png', bgr, [cv2.IMWRITE_PNG_COMPRESSION, 6])  # Higher compression
    return buffer.tobytes()

@dataclass
class AIResult:
    answers: Dict
    confidence: str
    notes: List
    success: bool
    student_code: Optional[str] = None

@dataclass
class StudentRecord:
    student_id: str
    name: str
    code: str

@dataclass
class GradingResult:
    student_id: str
    name: str
    detected_code: str
    score: int
    total: int
    page_number: int = 0

def analyze_with_ai(image_bytes, api_key, is_answer_key=True):
    """AI Analysis - optimized"""
    if not api_key or len(api_key) < 20:
        return AIResult({}, "no_api", ["API Key required"], False)
    
    try:
        import anthropic
        client = anthropic.Anthropic(api_key=api_key)
        
        image_b64 = base64.b64encode(image_bytes).decode('utf-8')
        
        if is_answer_key:
            prompt = "Ø§Ù‚Ø±Ø£ Answer Key. JSON ÙÙ‚Ø·: {\"answers\": {\"1\": \"C\", ...}}"
        else:
            prompt = """Ø£Ù†Øª Ù†Ø¸Ø§Ù… OMR Ø®Ø¨ÙŠØ±. Ø§Ù‚Ø±Ø£ ÙˆØ±Ù‚Ø© Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¨Ø¯Ù‚Ø©.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ Ø§Ù„ÙƒÙˆØ¯ (4 Ø£Ø±Ù‚Ø§Ù… Ø¨Ø§Ù„Ø¶Ø¨Ø·)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ø§Ù„ÙƒÙˆØ¯ ÙÙŠ Ø£Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ±Ù‚Ø© - Ø´Ø¨ÙƒØ© Ø£Ø±Ù‚Ø§Ù….
Ø§Ù‚Ø±Ø£ **4 ØµÙÙˆÙ ÙÙ‚Ø·** - ÙƒÙ„ ØµÙ = Ø±Ù‚Ù… ÙˆØ§Ø­Ø¯.
Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ØµØ­ÙŠØ­: **1000-1057**

Ù…Ø«Ø§Ù„ ØµØ­ÙŠØ­:
Ø§Ù„ØµÙ 1: "1" â†’ 1
Ø§Ù„ØµÙ 2: "0" â†’ 0
Ø§Ù„ØµÙ 3: "1" â†’ 1
Ø§Ù„ØµÙ 4: "3" â†’ 3
Ø§Ù„ÙƒÙˆØ¯ = "1013" âœ…

âŒ ØªØ¬Ù†Ø¨:
- Ø£ÙƒØ«Ø± Ù…Ù† 4 Ø£Ø±Ù‚Ø§Ù…
- Ø£Ù‚Ù„ Ù…Ù† 4 Ø£Ø±Ù‚Ø§Ù…
- Ø£ÙƒÙˆØ§Ø¯ Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚ 1000-1057

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© 1 - X ÙŠÙ„ØºÙŠ Ø§Ù„ÙÙ‚Ø§Ø¹Ø© (Ø£ÙˆÙ„ÙˆÙŠØ© Ù‚ØµÙˆÙ‰!):**
Q1: [â—X] A [â—] B [ ] C [ ] D
     Ù…Ù„Øº    âœ“
â†’ Ø§Ø­Ø°Ù A (Ø¹Ù„ÙŠÙ‡Ø§ X)
â†’ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: B âœ…

**Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© 2 - ÙÙ‚Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©:**
Q2: [ ] A [â—] B [ ] C [ ] D
â†’ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: B âœ…

**Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© 3 - Ø£ÙƒØ«Ø± Ù…Ù† ÙÙ‚Ø§Ø¹Ø©:**
Q3: [â—â—] A [â—] B [ ] C [ ] D
     Ø£ÙƒØ«Ø±   Ø£Ù‚Ù„
     Ù‚ØªØ§Ù…Ø©  Ù‚ØªØ§Ù…Ø©
â†’ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: A (Ø§Ù„Ø£ÙƒØ«Ø± Ù‚ØªØ§Ù…Ø©) âœ…

**Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©:**
1. Ø§Ø­Ø°Ù Ø£ÙŠ ÙÙ‚Ø§Ø¹Ø© Ø¹Ù„ÙŠÙ‡Ø§ X
2. Ù…Ù† Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ: Ø§Ø®ØªØ± Ø§Ù„Ø£ÙƒØ«Ø± Ù‚ØªØ§Ù…Ø©
3. Ø¥Ø°Ø§ Ù„Ø§ Ø´ÙŠØ¡: "?"

JSON ÙÙ‚Ø·:
{"student_code": "1013", "answers": {"1": "C", "2": "B", ...}}"""
        
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1500,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image", "source": {"type": "base64", "media_type": "image/png", "data": image_b64}},
                    {"type": "text", "text": prompt}
                ]
            }]
        )
        
        response_text = message.content[0].text
        
        import json, re
        json_text = response_text
        if "```json" in response_text:
            json_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            json_text = response_text.split("```")[1].split("```")[0].strip()
        
        try:
            result = json.loads(json_text)
        except:
            match = re.search(r'\{[\s\S]*\}', response_text)
            if match: result = json.loads(match.group())
            else: raise ValueError("No JSON")
        
        answers = {int(k): v for k, v in result.get("answers", {}).items()}
        student_code = result.get("student_code") if not is_answer_key else None
        
        return AIResult(answers, result.get("confidence", "medium"), result.get("notes", []), True, student_code)
    
    except Exception as e:
        return AIResult({}, "error", [str(e)], False)

def load_students_from_excel(file_bytes):
    """Load students from Excel"""
    try:
        df = pd.read_excel(io.BytesIO(file_bytes))
        id_col = name_col = code_col = None
        for col in df.columns:
            cl = str(col).lower().strip()
            if 'id' in cl or 'Ø±Ù‚Ù…' in cl: id_col = col
            elif 'name' in cl or 'Ø§Ø³Ù…' in cl: name_col = col
            elif 'code' in cl or 'ÙƒÙˆØ¯' in cl or 'Ø±Ù…Ø²' in cl: code_col = col
        
        if not all([id_col, name_col, code_col]):
            return []
        
        students = []
        for _, row in df.iterrows():
            students.append(StudentRecord(str(row[id_col]), str(row[name_col]), str(row[code_col])))
        return students
    except Exception as e:
        st.error(f"Excel error: {e}")
        return []

def find_student_by_code(students, code):
    """Find student with flexible matching"""
    code_norm = str(code).strip().replace(" ", "").replace("-", "")
    for s in students:
        s_code = str(s.code).strip().replace(" ", "").replace("-", "")
        if s_code == code_norm: return s
    
    # Try prefix match (if code is longer)
    if len(code_norm) > 4:
        for length in [4, 5, 6]:
            if len(code_norm) >= length:
                prefix = code_norm[:length]
                for s in students:
                    s_code = str(s.code).strip().replace(" ", "").replace("-", "")
                    if s_code == prefix: return s
    return None

def grade_student(student_answers, answer_key):
    """Grade student"""
    score = sum(1 for q in answer_key.keys() if student_answers.get(q) == answer_key[q])
    return score, len(answer_key)

def export_results(results):
    """Export to Excel - minimal format"""
    data = [{
        "Page": r.page_number,
        "ID": r.student_id, 
        "Name": r.name, 
        "Code": r.detected_code, 
        "Score": r.score
    } for r in results]
    
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        pd.DataFrame(data).to_excel(writer, sheet_name='Results', index=False)
    return output.getvalue()

# ==== MAIN APP ====
def main():
    st.set_page_config(page_title="ğŸ¤– AI OMR - Scalable", layout="wide")
    st.title("ğŸ¤– Ù†Ø¸Ø§Ù… OMR Ù„Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©")
    st.markdown("### ğŸ“Š 500-700 Ø·Ø§Ù„Ø¨ Ø¨Ø¯ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„!")
    
    # Session state
    if 'answer_key' not in st.session_state: st.session_state.answer_key = {}
    if 'students' not in st.session_state: st.session_state.students = []
    if 'results' not in st.session_state: st.session_state.results = []
    if 'processed_pages' not in st.session_state: st.session_state.processed_pages = set()
    if 'duplicate_warnings' not in st.session_state: st.session_state.duplicate_warnings = []
    if 'allow_duplicates' not in st.session_state: st.session_state.allow_duplicates = False
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        api_key = ""
        try:
            api_key = st.secrets.get("ANTHROPIC_API_KEY", "")
            if api_key: st.success("âœ… API Key")
        except: pass
        if not api_key:
            api_key = st.text_input("ğŸ”‘ API Key", type="password")
        
        st.markdown("---")
        st.metric("Answer Key", f"{len(st.session_state.answer_key)} Q")
        st.metric("Students", len(st.session_state.students))
        st.metric("Graded", len(st.session_state.results))
        
        if st.session_state.results:
            avg = np.mean([r.score/r.total*100 for r in st.session_state.results])
            st.metric("Average", f"{avg:.1f}%")
        
        if st.button("ğŸ”„ Reset All", type="secondary"):
            st.session_state.answer_key = {}
            st.session_state.results = []
            st.session_state.processed_pages = set()
            st.rerun()
    
    # Tabs
    tab1, tab2, tab3, tab4 = st.tabs(["1ï¸âƒ£ Answer Key", "2ï¸âƒ£ Students", "3ï¸âƒ£ Grade", "4ï¸âƒ£ Results"])
    
    # TAB 1: Answer Key
    with tab1:
        st.subheader("ğŸ“ Answer Key")
        key_file = st.file_uploader("Upload Answer Key", type=["pdf","png","jpg"], key="key")
        if key_file:
            if st.button("ğŸ¤– Analyze", type="primary"):
                if not api_key: 
                    st.error("âŒ Need API Key")
                else:
                    with st.spinner("Analyzing..."):
                        b = read_bytes(key_file)
                        pages = load_pages(b, key_file.name, 200)
                        if pages:
                            img = bgr_to_bytes(pil_to_bgr(pages[0]))
                            res = analyze_with_ai(img, api_key, True)
                            if res.success:
                                st.session_state.answer_key = res.answers
                                st.success(f"âœ… {len(res.answers)} questions")
                            else: st.error("Failed")
        
        if st.session_state.answer_key:
            st.info(" | ".join([f"Q{q}: {a}" for q, a in sorted(st.session_state.answer_key.items())]))
    
    # TAB 2: Students
    with tab2:
        st.subheader("ğŸ‘¥ Students")
        excel = st.file_uploader("Upload Excel (ID, Name, Code)", type=["xlsx","xls"], key="excel")
        if excel and st.button("ğŸ“Š Load"):
            students = load_students_from_excel(read_bytes(excel))
            if students:
                st.session_state.students = students
                st.success(f"âœ… {len(students)} students")
        
        if st.session_state.students:
            st.info(f"Loaded: {len(st.session_state.students)} students")
            with st.expander("View Students"):
                df = pd.DataFrame([{"ID": s.student_id, "Name": s.name, "Code": s.code} 
                                   for s in st.session_state.students[:50]])
                st.dataframe(df)
    
    # TAB 3: Grading
    with tab3:
        st.subheader("âœ… Grading - Optimized for Large Scale")
        
        if not st.session_state.answer_key:
            st.warning("âš ï¸ Load Answer Key first")
            return
        if not st.session_state.students:
            st.warning("âš ï¸ Load Students first")
            return
        
        st.info("""
        ğŸ’¡ **Ù„Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (500-700 Ø·Ø§Ù„Ø¨):**
        
        **Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§:**
        1. Ù‚Ø³Ù‘Ù… PDF Ø§Ù„ÙƒØ¨ÙŠØ± Ù„Ù…Ù„ÙØ§Øª Ø£ØµØºØ± (50-100 ÙˆØ±Ù‚Ø© Ù„ÙƒÙ„ Ù…Ù„Ù)
        2. Ø§Ø±ÙØ¹ Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©
        3. Ø¹Ø§Ù„Ø¬ 20-30 ÙˆØ±Ù‚Ø© ÙÙŠ ÙƒÙ„ Ø¯ÙØ¹Ø©
        4. Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØªØ¬Ù…Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
        
        **Ù…Ø«Ø§Ù„:** 500 Ø·Ø§Ù„Ø¨
        - Ø§Ù„Ù…Ù„Ù 1: Ø£ÙˆØ±Ø§Ù‚ 1-100 (10 Ø¯ÙØ¹Ø§Øª Ã— 10 Ø£ÙˆØ±Ø§Ù‚)
        - Ø§Ù„Ù…Ù„Ù 2: Ø£ÙˆØ±Ø§Ù‚ 101-200
        - Ø¥Ù„Ø®...
        
        **Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:** 5-7 Ù…Ù„ÙØ§Øª Ã— 5 Ø¯Ù‚Ø§Ø¦Ù‚ = 30-35 Ø¯Ù‚ÙŠÙ‚Ø©
        **Ø§Ù„ØªÙƒÙ„ÙØ©:** 500 Ã— $0.003 = $1.50
        """)
        
        sheets = st.file_uploader(
            "Ø§Ø±ÙØ¹ Ù…Ù„Ù PDF (Ù…ÙˆØµÙ‰ Ø¨Ù‡: 50-100 ÙˆØ±Ù‚Ø©)",
            type=["pdf"],
            accept_multiple_files=False,
            key="sheets"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            batch_size = st.slider("ğŸ“¦ Batch size", 5, 50, 20)
        with col2:
            auto_continue = st.checkbox("ğŸ”„ Auto-continue", value=True, help="Ø§Ø³ØªÙ…Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù„Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©")
        
        st.markdown("---")
        st.subheader("ğŸ” Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª")
        
        dup_mode = st.radio(
            "ÙƒÙŠÙ ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù…ÙƒØ±Ø±Ø©ØŸ",
            options=[
                "âš ï¸ ØªØ­Ø°ÙŠØ± ÙÙ‚Ø· (ØµØ­Ø­ Ø§Ù„Ø¬Ù…ÙŠØ¹)",
                "ğŸš« ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (ØµØ­Ø­ Ø§Ù„Ø£ÙˆÙ„ ÙÙ‚Ø·)",
                "âœ… Ù„Ø§ ØªÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (ØµØ­Ø­ ÙƒÙ„ Ø´ÙŠØ¡)"
            ],
            help="""
            **ØªØ­Ø°ÙŠØ± ÙÙ‚Ø·:** ÙŠØµØ­Ø­ ÙƒÙ„ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ ÙˆÙŠØ¹Ø·ÙŠÙƒ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù…ÙƒØ±Ø±Ø©
            **ØªØ¬Ø§Ù‡Ù„:** ÙŠØµØ­Ø­ Ø£ÙˆÙ„ ÙˆØ±Ù‚Ø© ÙÙ‚Ø· ÙˆÙŠØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¨Ø§Ù‚ÙŠ
            **Ù„Ø§ ØªÙØ­Øµ:** ÙŠØµØ­Ø­ ÙƒÙ„ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ (Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯Ùƒ Ø·Ù„Ø§Ø¨ Ø¨Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯)
            """
        )
        
        if st.session_state.duplicate_warnings:
            st.warning(f"âš ï¸ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(st.session_state.duplicate_warnings)} ÙƒÙˆØ¯ Ù…ÙƒØ±Ø±!")
            with st.expander("Ø¹Ø±Ø¶ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù…ÙƒØ±Ø±Ø©"):
                for dup in st.session_state.duplicate_warnings:
                    st.error(f"Ø§Ù„ÙƒÙˆØ¯ {dup['code']} - Ø§Ù„ØµÙØ­Ø§Øª: {', '.join(map(str, dup['pages']))}")
        
        if sheets and 'current_file_pages' not in st.session_state:
            if st.button("ğŸ” Load File"):
                with st.spinner("Loading file..."):
                    b = read_bytes(sheets)
                    pages = load_pages(b, sheets.name, 200)
                    st.session_state.current_file_pages = pages
                    st.session_state.current_file_idx = 0
                    st.success(f"âœ… Loaded {len(pages)} pages from {sheets.name}")
        
        if 'current_file_pages' in st.session_state:
            pages = st.session_state.current_file_pages
            current = st.session_state.current_file_idx
            total = len(pages)
            remaining = total - current
            
            st.metric("File Progress", f"{current}/{total} ({current/total*100:.0f}%)")
            
            if remaining > 0:
                if st.button(f"ğŸš€ Process next {min(batch_size, remaining)}", type="primary") or auto_continue:
                    end = min(current + batch_size, total)
                    
                    progress = st.progress(0)
                    status = st.empty()
                    
                    processed_count = 0
                    
                    for i in range(current, end):
                        rel = i - current
                        status.text(f"Page {i+1}/{total} ({rel+1}/{end-current})")
                        progress.progress((rel+1)/(end-current))
                        
                        # Skip if already processed
                        if i in st.session_state.processed_pages:
                            status.text(f"â­ï¸ Page {i+1} already processed")
                            continue
                        
                        page = pages[i]
                        bgr = pil_to_bgr(page)
                        img = bgr_to_bytes(bgr)
                        
                        res = analyze_with_ai(img, api_key, False)
                        
                        if not res.success or not res.student_code:
                            st.warning(f"âš ï¸ Page {i+1}: Failed to read")
                            continue
                        
                        code = res.student_code.strip()
                        
                        # Strict validation
                        if not code.isdigit():
                            st.warning(f"âš ï¸ Page {i+1}: Bad code '{code}' (contains non-digits)")
                            continue
                        
                        if len(code) != 4:
                            st.warning(f"âš ï¸ Page {i+1}: Bad code '{code}' (must be exactly 4 digits, got {len(code)})")
                            continue
                        
                        code_int = int(code)
                        if code_int < 1000 or code_int > 1999:
                            st.warning(f"âš ï¸ Page {i+1}: Code {code} out of range (expected 1000-1999)")
                            continue
                        
                        student = find_student_by_code(st.session_state.students, code)
                        if not student:
                            st.warning(f"âš ï¸ Page {i+1}: Code {code} not found in student list")
                            continue
                        
                        # Check for duplicates based on mode
                        already_graded = any(r.detected_code == code for r in st.session_state.results)
                        
                        if already_graded:
                            if "ØªØ¬Ø§Ù‡Ù„" in dup_mode:
                                # Mode 2: Skip duplicates
                                st.info(f"â„¹ï¸ Page {i+1}: Code {code} ({student.name}) already graded - skipping")
                                st.session_state.processed_pages.add(i)
                                continue
                            elif "ØªØ­Ø°ÙŠØ±" in dup_mode:
                                # Mode 1: Warn but continue grading
                                st.warning(f"âš ï¸ Page {i+1}: Code {code} is DUPLICATE - grading anyway")
                                
                                # Track duplicate
                                existing_dup = next((d for d in st.session_state.duplicate_warnings if d['code'] == code), None)
                                if existing_dup:
                                    existing_dup['pages'].append(i+1)
                                else:
                                    st.session_state.duplicate_warnings.append({
                                        'code': code,
                                        'name': student.name,
                                        'pages': [i+1]
                                    })
                            # Mode 3: No check - continues automatically
                        
                        score, total_q = grade_student(res.answers, st.session_state.answer_key)
                        
                        st.session_state.results.append(GradingResult(
                            student.student_id, student.name, code, score, total_q, i+1
                        ))
                        
                        st.session_state.processed_pages.add(i)
                        processed_count += 1
                        
                        status.text(f"âœ… Page {i+1}: {code} - {student.name} ({score}/{total_q})")
                        
                        # Free memory
                        del page, bgr, img
                    
                    st.session_state.current_file_idx = end
                    
                    # Force garbage collection
                    gc.collect()
                    
                    st.success(f"âœ… Processed {processed_count} pages")
                    
                    if end >= total:
                        st.balloons()
                        st.success("ğŸ‰ File complete!")
                        del st.session_state.current_file_pages
                        del st.session_state.current_file_idx
                    elif auto_continue:
                        time.sleep(1)
                        st.rerun()
            else:
                st.success("File complete! Upload next file or go to Results.")
    
    # TAB 4: Results
    with tab4:
        st.subheader("ğŸ“Š Results")
        
        if not st.session_state.results:
            st.info("No results yet")
            return
        
        # Duplicate warnings section
        if st.session_state.duplicate_warnings:
            st.error(f"âš ï¸ **ØªØ­Ø°ÙŠØ±: ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(st.session_state.duplicate_warnings)} ÙƒÙˆØ¯ Ù…ÙƒØ±Ø±!**")
            
            with st.expander("ğŸ” ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù…ÙƒØ±Ø±Ø©", expanded=True):
                st.markdown("""
                **Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø¸Ù‡Ø±Øª ÙÙŠ Ø£ÙƒØ«Ø± Ù…Ù† ÙˆØ±Ù‚Ø©:**
                - Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø·Ø§Ù„Ø¨ ÙƒØªØ¨ ÙƒÙˆØ¯ Ø²Ù…ÙŠÙ„Ù‡ Ø¨Ø§Ù„Ø®Ø·Ø£
                - Ø±Ø§Ø¬Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ ÙŠØ¯ÙˆÙŠØ§Ù‹
                - ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ÙˆØ§Ù„Ø®Ø·
                """)
                
                for dup in st.session_state.duplicate_warnings:
                    st.warning(f"""
                    **Ø§Ù„ÙƒÙˆØ¯:** {dup['code']} - **Ø§Ù„Ø§Ø³Ù…:** {dup['name']}  
                    **Ø¸Ù‡Ø± ÙÙŠ Ø§Ù„ØµÙØ­Ø§Øª:** {', '.join(map(str, dup['pages']))}  
                    **Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª:** {len(dup['pages'])} Ù…Ø±Ø©
                    """)
                
                # Show affected results
                dup_codes = [d['code'] for d in st.session_state.duplicate_warnings]
                dup_results = [r for r in st.session_state.results if r.detected_code in dup_codes]
                
                if dup_results:
                    st.markdown("**Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©:**")
                    dup_df = pd.DataFrame([{
                        "Page": r.page_number,
                        "Code": r.detected_code,
                        "Name": r.name,
                        "Score": r.score
                    } for r in dup_results])
                    st.dataframe(dup_df, width='stretch')
            
            st.markdown("---")
        
        scores = [r.score/r.total*100 for r in st.session_state.results]
        col1, col2, col3, col4 = st.columns(4)
        with col1: st.metric("Graded", len(scores))
        with col2: st.metric("Average", f"{np.mean(scores):.1f}%")
        with col3: st.metric("Max", f"{np.max(scores):.1f}%")
        with col4: st.metric("Min", f"{np.min(scores):.1f}%")
        
        df = pd.DataFrame([{
            "Page": r.page_number,
            "ID": r.student_id, 
            "Name": r.name, 
            "Code": r.detected_code,
            "Score": r.score,
            "%": f"{r.score/r.total*100:.0f}"
        } for r in st.session_state.results])
        
        st.dataframe(df, width='stretch')
        
        # Duplicate cleaning options
        if st.session_state.duplicate_warnings:
            st.markdown("---")
            st.subheader("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª")
            
            clean_method = st.radio(
                "ÙƒÙŠÙ ØªØ±ÙŠØ¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§ØªØŸ",
                options=[
                    "Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø£ÙˆÙ„ ÙÙ‚Ø·",
                    "Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø©",
                    "Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø£Ù‚Ù„ Ø¯Ø±Ø¬Ø© (Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©)",
                    "Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø¬Ù…ÙŠØ¹ (Excel Ø³ÙŠØ¸Ù‡Ø± ÙƒÙ„Ù‡Ù…)"
                ]
            )
            
            if st.button("ğŸ§¹ Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªØ§Ø¦Ø¬ Ù†Ø¸ÙŠÙØ©"):
                if "Ø§Ù„Ø£ÙˆÙ„" in clean_method:
                    # Keep first occurrence
                    clean_results = []
                    seen_codes = set()
                    for r in sorted(st.session_state.results, key=lambda x: x.page_number):
                        if r.detected_code not in seen_codes:
                            clean_results.append(r)
                            seen_codes.add(r.detected_code)
                    st.success(f"âœ… ØªÙ…! {len(clean_results)} Ù†ØªÙŠØ¬Ø© Ù†Ø¸ÙŠÙØ© (Ø­Ø°Ù {len(st.session_state.results) - len(clean_results)} ØªÙƒØ±Ø§Ø±)")
                    st.session_state.clean_results = clean_results
                
                elif "Ø§Ù„Ø£Ø¹Ù„Ù‰" in clean_method:
                    # Keep highest score
                    from collections import defaultdict
                    by_code = defaultdict(list)
                    for r in st.session_state.results:
                        by_code[r.detected_code].append(r)
                    
                    clean_results = []
                    for code, results in by_code.items():
                        best = max(results, key=lambda x: x.score)
                        clean_results.append(best)
                    st.success(f"âœ… ØªÙ…! {len(clean_results)} Ù†ØªÙŠØ¬Ø© (Ø£ÙØ¶Ù„ Ø¯Ø±Ø¬Ø© Ù„ÙƒÙ„ ÙƒÙˆØ¯)")
                    st.session_state.clean_results = clean_results
                
                elif "Ø§Ù„Ø£Ù‚Ù„" in clean_method:
                    # Keep lowest score (for review)
                    from collections import defaultdict
                    by_code = defaultdict(list)
                    for r in st.session_state.results:
                        by_code[r.detected_code].append(r)
                    
                    clean_results = []
                    for code, results in by_code.items():
                        worst = min(results, key=lambda x: x.score)
                        clean_results.append(worst)
                    st.success(f"âœ… ØªÙ…! {len(clean_results)} Ù†ØªÙŠØ¬Ø© (Ø£Ù‚Ù„ Ø¯Ø±Ø¬Ø© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©)")
                    st.session_state.clean_results = clean_results
                
                else:
                    # Keep all
                    st.session_state.clean_results = st.session_state.results
        
        # Export buttons
        st.markdown("---")
        results_to_export = st.session_state.get('clean_results', st.session_state.results)
        
        if st.button("ğŸ“¥ Export Excel", type="primary"):
            excel = export_results(results_to_export)
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            status_text = f"({len(results_to_export)} Ù†ØªÙŠØ¬Ø©"
            if 'clean_results' in st.session_state and len(results_to_export) < len(st.session_state.results):
                status_text += f" - ØªÙ… ØªÙ†Ø¸ÙŠÙ {len(st.session_state.results) - len(results_to_export)} ØªÙƒØ±Ø§Ø±"
            status_text += ")"
            
            st.download_button(
                f"â¬‡ï¸ Download {status_text}", 
                excel, 
                f"results_{ts}.xlsx", 
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

if __name__ == "__main__":
    main()
